{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise 7: Deep Learning 1 (Part B)\n",
    "In this notebook we will gain some hands-on experience with backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)\n",
    "# Convert labels into one-hot format\n",
    "Y = label_binarize(y, classes=np.unique(y))\n",
    "K = Y.shape[1]  # number of classes\n",
    "D = X.shape[1]  # number of features\n",
    "\n",
    "np.random.seed(123)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1257, 64) (1257, 10) (540, 64) (540, 10)\n",
      "[ 0.  1. 10. 16. 16.  8.  0.  0.  0. 10. 16. 13. 16. 12.  0.  0.  0.  1.\n",
      "  3.  3. 16.  9.  0.  0.  0.  0.  0. 13. 14.  1.  0.  0.  0.  0.  2. 16.\n",
      " 16. 12.  3.  0.  0.  0.  0.  5. 11. 16. 11.  0.  0.  0.  2.  7. 14. 16.\n",
      "  6.  0.  0.  0. 11. 16. 13.  5.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simple backpropagation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the d_out means the gradient of the loss function with respect to the output of the layer\n",
    "\n",
    "class Add:\n",
    "    def forward(self, x, y):\n",
    "        # TODO\n",
    "        z = x + y\n",
    "        return z\n",
    "    def backward(self, d_out):\n",
    "        # TODO\n",
    "        dx = d_out\n",
    "        dy = d_out\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise multiplication of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply:\n",
    "    def forward(self, x, y):\n",
    "        # TODO\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        z = x * y\n",
    "        return z\n",
    "      \n",
    "    def backward(self, d_out):\n",
    "        # TODO\n",
    "        dx = d_out * self.y\n",
    "        dy = d_out * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum:\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        z = np.sum(x)    \n",
    "        return z\n",
    "    def backward(self, d_out):\n",
    "        # TODO\n",
    "        return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product of two vectors as composition of multiplication and summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product:\n",
    "$$\\mathbf{x}\\cdot\\mathbf{y} = \\sum_{i=0}^{n}x_i y_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 5, dtype=np.float32)\n",
    "y = np.arange(-1, 3, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4.]\n",
      "[-1.  0.  1.  2.]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult = Multiply()\n",
    "vec_sum = Sum()\n",
    "\n",
    "w = mult.forward(x, y)\n",
    "z = vec_sum.forward(w)\n",
    "\n",
    "d_w = vec_sum.backward(1.0)\n",
    "d_x, d_y = mult.backward(d_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0,\n",
       " array([-1.,  0.,  1.,  2.], dtype=float32),\n",
       " array([1., 2., 3., 4.], dtype=float32))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, d_x, d_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product of two vectors as one operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProduct:\n",
    "    def forward(self, x, y):\n",
    "        # TODO\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        z = np.dot(x, y)\n",
    "        return z\n",
    "    def backward(self, d_out):\n",
    "        # TODO\n",
    "        dx = d_out * self.y\n",
    "        dy = d_out * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 5, dtype=np.float32)\n",
    "y = np.arange(-1, 3, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DotProduct()\n",
    "z = dp.forward(x, y)\n",
    "d_x, d_y = dp.backward(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0,\n",
       " array([-1.,  0.,  1.,  2.], dtype=float32),\n",
       " array([1., 2., 3., 4.], dtype=float32))"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, d_x, d_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lessons:**\n",
    "1. By implementing `forward` and `backward` method we can compute gradients of an arbitrary composition of functions\n",
    "2. Use `cache` to store values that will be needed in the backward \n",
    "3. Multiple operations can be combined into a single module (i.e. function)\n",
    "4.  `1.0` as `d_out` for the terminal node in our computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Multi-class logistic regression (without backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-class logistic regression model\n",
    "\n",
    "Data:\n",
    "* Data matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$.\n",
    "* Target labels in one-hot format $\\mathbf{Y} \\in \\mathbb{R}^{N \\times K}$.\n",
    "$Y_{nk} = 1$ if sample $n$ belongs to class $k$, $Y_{nk} = 0$ otherwise.\n",
    "\n",
    "Model parameters:\n",
    "* Weight matrix $\\mathbf{W} \\in \\mathbb{R}^{D \\times K}$.\n",
    "* Bias vector $\\mathbf{b} \\in \\mathbb{R}^{K}$.\n",
    "\n",
    "Making predictions with the model:\n",
    "* Logits \n",
    "$$\\mathbf{a}_n = \\mathbf{W x}_n + \\mathbf{b}$$\n",
    "* Denote the matrix of logits as \n",
    "$$\\mathbf{A} = \\mathbf{XW} +  \\mathbf{1}_N \\mathbf{b}^T \\in \\mathbb{R}^{N \\times K}$$\n",
    "* Convert logits to probabilities using softmax function\n",
    "$$p(Y_{nk} = 1 \\mid \\mathbf{x}_n, \\mathbf{W}, \\mathbf{b}) = \\frac{\\exp(A_{nk})}{\\sum_{c = 1}^{K} \\exp(A_{nc})}$$\n",
    "\n",
    "Negative log-likelihood\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "-\\log p(\\mathbf{Y} \\mid \\mathbf{X}, \\mathbf{W}, \\mathbf{b}) &= - \\frac{1}{N}\\sum_{n=1}^{N} \\sum_{k=1}^{K} Y_{nk} \\log p(Y_{nk} = 1 \\mid \\mathbf{x}_n, \\mathbf{W}, \\mathbf{b})\\\\\n",
    "&= \\frac{1}{N} \\sum_{n=1}^{N} \\sum_{k=1}^{K} Y_{nk} \\left(-A_{nk} + \\log \\left( \\sum_{c=1}^{C} \\exp(A_{nc}) \\right) \\right)\\\\\n",
    "%&= \\frac{1}{N} \\sum_{n=1}^{N} \\left(\\sum_{k=1}^{K} -Y_{nk} A_{nk} \\right) + \\log \\left( \\sum_{c=1}^{C} \\exp(A_{nc}) \\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    \"\"\"Generate predictions for a multi-class logistic regression model.\n",
    "\n",
    "    Args:\n",
    "        X: data matrix, shape (N, D)\n",
    "        W: weight matrix, shape (D, K)\n",
    "        b: bias vector, shape (K)\n",
    "\n",
    "    Returns:\n",
    "        Y_pred: Predicted class probabilities, shape (N, K).\n",
    "            Y_pred[n, k] = probability that sample n belongs to class k.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    logist = np.dot(X, W) + b\n",
    "    Y_pred = softmax(logist, axis=1)\n",
    "   \n",
    "    return Y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative log-likelihood of multiclass logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(X, W, b, Y):\n",
    "    \"\"\"Compute negative log-likelihood of a logistic regression model.\n",
    "\n",
    "    Also known as categorical cross entropy loss.\n",
    "\n",
    "    Args:\n",
    "        X: data matrix, shape (N, D)\n",
    "        W: weight matrix, shape (D, K)\n",
    "        b: bias vector, shape (K)\n",
    "        Y: true labels in one-hot format, shape (N, K)\n",
    "\n",
    "    Returns:\n",
    "        loss: loss of the logistic regression model, shape ()\n",
    "    \"\"\"\n",
    "    e = 1e-9 # for numerical stability\n",
    "    loss = -np.sum(Y * np.log(predict(X, W, b) + e)) / X.shape[0]\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_grad(X, W, b, Y):\n",
    "    \"\"\"Compute gradient of the NLL loss w.r.t. W and b.\n",
    "\n",
    "    Args:\n",
    "        X: data matrix, shape (N, D)\n",
    "        W: weight matrix, shape (D, K)\n",
    "        b: bias vector, shape (K)\n",
    "        Y: true labels in one-hot format, shape (N, K)\n",
    "\n",
    "    Returns:\n",
    "        d_W: gradient of the los w.r.t. W, shape (D, K)\n",
    "        d_b: gradient of the los w.r.t. b, shape (K)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    \n",
    "    p = predict(X, W, b)\n",
    "    \n",
    "    d_W = np.dot(X.T, p - Y) / x.shape[0]\n",
    "    d_b = np.sum(p - Y, axis=0) / x.shape[0]\n",
    "    \n",
    "    return d_W, d_b\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize learnable model parameters\n",
    "W = np.zeros([D, K])\n",
    "b = np.zeros([K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimization parameters\n",
    "learning_rate = 1e-2\n",
    "max_epochs = 250\n",
    "report_frequency = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.302585082994045\n",
      "Epoch 25, loss: 1.4892111715536795\n",
      "Epoch 50, loss: 0.6537324967243424\n",
      "Epoch 75, loss: 0.34199151086644314\n",
      "Epoch 100, loss: 0.20548972104264995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, loss: 0.16044226924290544\n",
      "Epoch 150, loss: 0.1108155967836138\n",
      "Epoch 175, loss: 0.06649041011266495\n",
      "Epoch 200, loss: 0.03642813028326075\n",
      "Epoch 225, loss: 1.7924880261156382e-05\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    # Compute train loss\n",
    "    # TODO\n",
    "    train_loss = nll_loss(X_train, W, b, Y_train)\n",
    "    \n",
    "    # Print train loss every `report_frequency` epochs\n",
    "    # TODO\n",
    "    \n",
    "    if epoch % report_frequency == 0:\n",
    "        print(f'Epoch {epoch}, loss: {train_loss}')\n",
    "\n",
    "    # Perform the update\n",
    "    # TODO\n",
    "    d_W, d_b = nll_grad(X_train, W, b, Y_train)\n",
    "    W -= learning_rate * d_W\n",
    "    b -= learning_rate * d_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.9024807872890447\n",
      "Test accuracy: 0.9537037037037037\n"
     ]
    }
   ],
   "source": [
    "# Compute test loss\n",
    "# TODO\n",
    "test_loss = nll_loss(X_test, W, b, Y_test)\n",
    "\n",
    "# Compute test accuracy\n",
    "# TODO\n",
    "test_accuracy = accuracy_score(Y_test.argmax(axis=1), predict(X_test, W, b).argmax(axis=1))\n",
    "\n",
    "# Print test loss and accuracy\n",
    "# TODO\n",
    "print(f'Test loss: {test_loss}')\n",
    "print(f'Test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-class logistic regression (with backprop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.W = np.random.randn(input_dim, output_dim)\n",
    "        self.b = np.random.randn(output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        return z\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        d_x = np.dot(d_out, self.W.T)\n",
    "        d_W = np.dot(self.x.T, d_out)\n",
    "        d_b = np.sum(d_out, axis=0)\n",
    "        return d_x, d_W, d_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropy:\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        e = 1e-9\n",
    "        loss = -np.sum(y * np.log(x + e)) / x.shape[0]\n",
    "        return loss\n",
    "    \n",
    "    def backward(self):\n",
    "        d_x = -self.y / (self.x + 1e-9) / self.x.shape[0]\n",
    "        return d_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"Logistic regression model.\n",
    "\n",
    "    Gradients are computed with backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, num_classes, learning_rate=1e-2):\n",
    "        # Initialize hyperparameters\n",
    "        # TODO\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialize the model parameters\n",
    "        # TODO\n",
    "        self.W = np.zeros([num_features, num_classes])\n",
    "        self.b = np.zeros([num_classes])\n",
    "        self.losses = []\n",
    "            \n",
    "        # Define layers\n",
    "        # TODO\n",
    "        self.dot_product = DotProduct()\n",
    "        self.linear = Linear(num_features, num_classes)\n",
    "        \n",
    "        # Define loss\n",
    "        # TODO\n",
    "        self.loss = CategoricalCrossEntropy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate predictions for one minibatch.\n",
    "\n",
    "        Args:\n",
    "            X: data matrix, shape (N, D)\n",
    "\n",
    "        Returns:\n",
    "            Y_pred: predicted class probabilities, shape (N, D)\n",
    "            Y_pred[n, k] = probability that sample n belongs to class k\n",
    "        \"\"\"\n",
    "        # TODO        \n",
    "        logist = self.linear.forward(X)\n",
    "        Y_pred = softmax(logist, axis=1)\n",
    "        return Y_pred\n",
    "        \n",
    "    def step(self, X, Y):\n",
    "        \"\"\"Perform one step of gradient descent on the minibatch of data.\"\"\"\n",
    "        # Forward  - compute the loss on training data\n",
    "        # TODO\n",
    "        loss = self.loss.forward(self.predict(X), Y)\n",
    "        self.losses.append(loss)\n",
    "        \n",
    "        # Backward  - compute the gradients of loss w.r.t. all the model parameters\n",
    "        # TODO\n",
    "        d_x, d_W, d_b = self.linear.backward(d_x)\n",
    "        \n",
    "        # Apply the gradients\n",
    "        # TODO\n",
    "        \n",
    "        self.W -= self.learning_rate * d_W\n",
    "        self.b -= self.learning_rate * d_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify optimization parameters\n",
    "learning_rate = 1e-2\n",
    "max_epochs = 301\n",
    "report_frequency = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(num_features=D, num_classes=K, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'd_x' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[325], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Perform one step of gradient descent\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     log_reg\u001b[38;5;241m.\u001b[39mstep(X_train, Y_train)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Print train loss every `report_frequency` epochs\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m report_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[321], line 53\u001b[0m, in \u001b[0;36mLogisticRegression.step\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Backward  - compute the gradients of loss w.r.t. all the model parameters\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m d_x, d_W, d_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mbackward(d_x)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Apply the gradients\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m d_W\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'd_x' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "for epoch in range(max_epochs):\n",
    "    # Perform one step of gradient descent\n",
    "    # TODO\n",
    "    log_reg.step(X_train, Y_train)\n",
    "\n",
    "    # Print train loss every `report_frequency` epochs\n",
    "    # TODO\n",
    "    if epoch % report_frequency == 0:\n",
    "        print(f'Epoch {epoch}, loss: {train_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute test loss\n",
    "# TODO\n",
    "\n",
    "# Compute test accuracy\n",
    "# TODO\n",
    "\n",
    "# Print test loss and accuracy\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
